import requests
from bs4 import BeautifulSoup
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI
import numpy as np
from typing import List, Dict
from supabase_writer import save_to_supabase

print("üéâ rag_core „Åå„É≠„Éº„Éâ„Åï„Çå„Åæ„Åó„Åü")

# ============ 0. OpenAI„ÇØ„É©„Ç§„Ç¢„É≥„Éà ============
client = OpenAI()  # Áí∞Â¢ÉÂ§âÊï∞ OPENAI_API_KEY „ÇíÂà©Áî®

# ======== „Ç∞„É≠„Éº„Éê„É´Ôºà„É°„É¢„É™ÂÜÖDBÔºâ =========
CHUNKS: List[Dict] = []          # {"source": str, "text": str}
EMBEDDINGS: List[np.ndarray] = []


# ============ 1. „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ÔºàWeb„Éö„Éº„Ç∏Ôºâ ============
def load_web_urls(urls: List[str]) -> List[Dict]:
    docs = []
    for url in urls:
        print(f"üìò WebÂèñÂæó‰∏≠: {url}")
        html = requests.get(url, timeout=20).text
        soup = BeautifulSoup(html, "lxml")
        for tag in soup(["script", "style", "header", "footer", "nav"]):
            tag.decompose()
        text = soup.get_text(separator="\n")
        docs.append({"source": url, "text": text})
    return docs


# ============ 2. PDFÂèñ„ÇäËæº„Åø ============
def load_pdfs(paths: List[str]) -> List[Dict]:
    docs = []
    for pdf in paths:
        print(f"üìï PDFË™≠„ÅøËæº„Åø‰∏≠: {pdf}")
        reader = PdfReader(pdf)
        txt = ""
        for page in reader.pages:
            page_text = page.extract_text() or ""
            txt += page_text + "\n"
        docs.append({"source": pdf, "text": txt})
    return docs


# ============ 3. „ÉÅ„É£„É≥„ÇØÂåñ ============
def chunk_docs(documents: List[Dict]) -> List[Dict]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200
    )
    chunks = []
    for d in documents:
        for chunk in splitter.split_text(d["text"]):
            chunks.append({"source": d["source"], "text": chunk})
    return chunks


# ============ 4. OpenAI Âüã„ÇÅËæº„Åø ============
def embed(text: str) -> np.ndarray:
    res = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return np.array(res.data[0].embedding)


# ============ 5. È°û‰ººÊ§úÁ¥¢ ============
def search(query: str, top_k: int = 3):
    if not CHUNKS or not EMBEDDINGS:
        return []
    q_emb = embed(query)
    scores = []
    qn = np.linalg.norm(q_emb)
    for i, emb in enumerate(EMBEDDINGS):
        score = float(np.dot(q_emb, emb) / (qn * np.linalg.norm(emb)))
        scores.append((score, i))
    scores.sort(reverse=True)
    top = scores[:top_k]
    return [ (CHUNKS[i], s) for s, i in top ]


# ============ 6. LLM „ÅßÂõûÁ≠îÁîüÊàêÔºàRAGÔºâ ============
def answer(query: str, retrieved_docs: List):
    context = "\n\n".join([d["text"] for d, _ in retrieved_docs])
    prompt = f"""
„ÅÇ„Å™„Åü„ÅØ„ÄåÂÉç„Åè„ÅÇ„Åï„Å≤„Åã„ÇèÔºàhataraku-asahikawa.jpÔºâ„ÄçÂ∞ÇÁî®„ÅÆÊ°àÂÜÖ„ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà„Åß„Åô„ÄÇ
‰ª•‰∏ã„ÅÆË≥áÊñô„Å®„Ç¶„Çß„Éñ„Çµ„Ç§„ÉàÊÉÖÂ†±„Å†„Åë„ÇíÊ†πÊã†„Å´ÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

# Ë≥áÊñô
{context}

# Ë≥™Âïè
{query}

# ÂõûÁ≠îÔºàÁ´ØÁöÑ„Å´Ôºâ
"""
    res = client.chat.completions.create(
        model="gpt-4.1-mini",  # ‚Üê„ÅÇ„Å™„Åü„ÅÆÊåáÂÆö„Å©„Åä„Çä
        messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content


# ============ 7. „Ç§„É≥„Éá„ÇØ„Ç∑„É≥„Ç∞ÔºàÂÖ¨ÈñãÈñ¢Êï∞Ôºâ ============
def build_index(web_urls: List[str], pdf_paths: List[str]) -> int:
    global CHUNKS, EMBEDDINGS

    web_docs = load_web_urls(web_urls) if web_urls else []
    pdf_docs = load_pdfs(pdf_paths) if pdf_paths else []
    all_docs = web_docs + pdf_docs

    new_chunks = chunk_docs(all_docs)
    new_embeddings = [embed(c["text"]) for c in new_chunks]

    CHUNKS.extend(new_chunks)
    EMBEDDINGS.extend(new_embeddings)

    # ‚òÖ Supabase„Å∏‰øùÂ≠ò
    for chunk, emb in zip(new_chunks, new_embeddings):
        res = save_to_supabase(chunk["text"], emb)
        print("Saved:", res)

    return len(new_chunks)